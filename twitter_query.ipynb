{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "# handle with care!\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Used this for twitter API, no need for it now!\n",
    "# consumer_key = 'lcGh2Z7t1BQEOdpqn2S1Jce8O'\n",
    "# consumer_secret = 'IyJh2frDC6Xb7M1FV68v4qLHtOTRv23XKHWoRZuchpC1dtSwop'\n",
    "# access_token = '15233837-i5Rh8eWV5OhpEWeSBIyhoxNxEMaCrMUOSsg932F5B'\n",
    "# access_token_secret = 'Di6cPb6CIpwZX0kQ4GjVm5j7j6lssarlwGwTca8l3a0bs'\n",
    "\n",
    "\n",
    "# auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "# auth.set_access_token(access_token, access_token_secret)\n",
    "# api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "\n",
    "dubai_gov = ['@DXBMediaOffice',\n",
    "             '@SmartDubai', '@dghr_gov', '@DMunicipality', '@LAD_Dubai', '@CDA_Dubai', '@DHA_Dubai',\n",
    "             '@KHDA', '@Dubai_Ambulance', '@dubaimediainc', '@DubaiWomenEst', '@NedaaDXB',\n",
    "             '@DubaiPoliceHQ', '@GDRFADUBAI', '@DubaiStatistics']\n",
    "\n",
    "kuwait_gov = ['@SocialDOffice', '@KuwaitiCM', '@CitraKuwait', '@GPFKW', '@CAIT_KW', '@CSBkw', '@Csc_Kw', '@Moi_kuw',\n",
    "              '@KUWAIT_MOH', '@mosal_q8', '@MOInformation', '@mockuwait', '@mpw_kw', '@youthkuwait', '@MajlesAlOmmah',\n",
    "              '@kuwmun', '@OfficialKfsd', '@manpower_kwt', '@kuna_ar', '@NewKuwaitGov']\n",
    "\n",
    "# check handles for twitter\n",
    "kuwait_influencers = ['@Alafasy', '@NabilAlawadhy', '@DrAlnefisi', '@TfTeeeSH', '@mbuyabis', '@safaalhashem', '@MarzouqAlghanim',\n",
    "                      '@FaisalAlBasri', '@bourashed', '@nohastyleicon', '@Bibii6363', '@Dr__ALKANDARI', '@ThamerAlsuwait', '@WahabAlbabtain',\n",
    "                      '@ladyrainq8', '@Tabtabaee', '@Ran01da', '@MHALHAJARAF', '@alhuwaila', '@BO3LOSH81', '@Abdllah_fhhad', '@watan_alnhhar',\n",
    "                      '@ferasaloan', '@OdahAlodah', '@bntblad29160144', '@Mishari77', '@fatma_aljenaei', '@kuwait_PLg', '@saeedtawfiqi',\n",
    "                      '@NasrAlDousari', '@kuwait00660830', '@kuniv_j', '@AlroumiDr', '@Thephoenix_24'\n",
    "                      ]\n",
    "                      \n",
    "dubai_influencers = [\"@mustafa_agha\",\n",
    "                     \"@Shamma_Hamdan\",\n",
    "                     \"@Lara\",\n",
    "                     \"@Lojain_omran\",\n",
    "                     \"@mobeid\",\n",
    "                     \"@dbelhoul\",\n",
    "                     \"@WazhmaAyoubi\",\n",
    "                     \"@LSLofficial\",\n",
    "                     \"@DubaiNameShame\",\n",
    "                     \"@TwoToneDXB\",\n",
    "                     \"@krisfade\",\n",
    "                     \"@Mahra\",\n",
    "                     \"@h_alrahoomi\",\n",
    "                     \"@DJBLISS\",\n",
    "                     \"@_SocialCompany\",\n",
    "                     \"@M_BinTheneya\",\n",
    "                     \"@afmfilms\",\n",
    "                     \"@HishamWyne\",\n",
    "                     \"@zizzleblog\",\n",
    "                     \"@literallyanika\",\n",
    "                     \"@1magicphil\",\n",
    "                     \"@Sara_Falaknaz\",\n",
    "                     \"@GrosvenorHouse\",\n",
    "                     \"@OsamaAlshafar\",\n",
    "                     \"@FryingPanTours\",\n",
    "                     \"@EatDrinkStayDXB\",\n",
    "                     \"@RiggsGolf\",\n",
    "                     \"@GLORYGIRLFIT\",\n",
    "                     \"@samiraOlfat\",\n",
    "                     \"@farhanabodi\",\n",
    "                     \"@HassanSoukar\",\n",
    "                     \"@amrutant\",\n",
    "                     \"@Cookwithzahra\",\n",
    "                     \"@ayeina_official\",\n",
    "                     \"@BloomerBrittany\",\n",
    "                     ]\n",
    "\n",
    "since_date = '2020-01-03'\n",
    "until_date = '2022-06-03'\n",
    "count = 5000\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet scraper for single user and single keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_query_tweets(username, text_query, count, since_date, until_date, lang):\n",
    "    tweetlist2 = []\n",
    "    # print('from:'+username+' '+text_query+' since:'+since_date+' until:'+until_date)\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper('from:'+username+' '+text_query+' since:'+since_date+' until:'+until_date +' lang:'+lang).get_items()):\n",
    "        if i > count:\n",
    "            break\n",
    "        if tweet.content:\n",
    "            tweetlist2.append([tweet.date,\n",
    "                               tweet.id,\n",
    "                               tweet.rawContent,\n",
    "                               tweet.user.username,\n",
    "                               tweet.inReplyToUser,\n",
    "                            #    tweet.inReplyToTweetId,\n",
    "                               tweet.replyCount,\n",
    "                               tweet.retweetCount,\n",
    "                               tweet.likeCount,\n",
    "                               tweet.user.followersCount,\n",
    "                               tweet.user.friendsCount,\n",
    "                               tweet.user.statusesCount,\n",
    "                               tweet.lang])\n",
    "    # Creating a dataframe from the tweets list above\n",
    "    tweets_df2 = pd.DataFrame(\n",
    "        tweetlist2, columns=['Datetime',\n",
    "                             'Tweet Id',\n",
    "                             'Text',\n",
    "                             'Username',\n",
    "                             'in_reply_to_user_id',\n",
    "                            #  'in_reply_to_status_id',\n",
    "                             'reply_count',\n",
    "                             'retweet_count',\n",
    "                             'favorite_count',\n",
    "                             'follower_count',\n",
    "                             'friends_count',\n",
    "                             'total_tweets',\n",
    "                             'lang'])\n",
    "    return tweets_df2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check twitter handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'@Lara': False, '@LSLofficial': False, '@DubaiNameShame': False, '@TwoToneDXB': False, '@krisfade': False, '@DJBLISS': False, '@_SocialCompany': False, '@HishamWyne': False, '@zizzleblog': False, '@literallyanika': False, '@1magicphil': False, '@GrosvenorHouse': False, '@FryingPanTours': False, '@EatDrinkStayDXB': False, '@RiggsGolf': False, '@GLORYGIRLFIT': False, '@samiraOlfat': False, '@farhanabodi': False, '@amrutant': False, '@Cookwithzahra': False, '@ayeina_official': False, '@BloomerBrittany': False}\n"
     ]
    }
   ],
   "source": [
    "check_users = []\n",
    "lang = 'ar'\n",
    "for user in dubai_influencers:\n",
    "    df = user_query_tweets(user, '',1, since_date, until_date,lang)\n",
    "    check_users.append(df.shape[0] > 0)\n",
    "\n",
    "res = dict(zip(dubai_influencers, check_users))\n",
    "#print false ones\n",
    "print({k: v for (k,v) in res.items() if v is False})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Twitter Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_info = ['Datetime',\n",
    "                             'Tweet Id',\n",
    "                             'Text',\n",
    "                             'Username',\n",
    "                             'in_reply_to_user_id',\n",
    "                            #  'in_reply_to_status_id',\n",
    "                             'reply_count',\n",
    "                             'retweet_count',\n",
    "                             'favorite_count',\n",
    "                             'follower_count',\n",
    "                             'friends_count',\n",
    "                             'total_tweets',\n",
    "                             'lang']\n",
    "\n",
    "\n",
    "def user_query_tweets_to_csv(file_path,username_list,query_terms,count,since_date,until_date, lang):\n",
    "    tweets_df=pd.DataFrame(list(), columns=tweet_info)\n",
    "    tweet_cnt = []\n",
    "    for i,query in enumerate(query_terms):\n",
    "        for ii,name in enumerate(username_list):\n",
    "            data=user_query_tweets(name ,query,count,since_date,until_date, lang)\n",
    "            if not data.empty:\n",
    "                tweets_df=pd.concat([tweets_df, data], axis = 0)\n",
    "                tweet_cnt.append([query, name, data.shape[0]])\n",
    "\n",
    "    tweets_df.to_csv(file_path,index=False,encoding='utf-8')\n",
    "    return tweets_df, tweet_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_terms = ['کورونا','كوفيد','Covid-19','العمال غير المواطنين','Covid','مواطن','المغتربين',\n",
    "#                'منظمة غير حكومية','تمكين المواطن','التنمية المجتمعية المستدامة','الدمقرطة',\n",
    "#               'منظمة الناس','مشاركة المواطن','المشاركة المدنية','سياسة المجتمع','بناء القدرات',\n",
    "#               'منظمة شعبية','الرأسمالية الاجتماعية','التنمية التشاركية','حقوق التصويت','التعددية','مجتمع مدني مستقل'\n",
    "#                ,'المناصرة','العدالة البديلة','التأثير العام','الشفافية السياسية','الشفافية الاقتصادية','التربية المدنية']\n",
    "\n",
    "query_terms_ar = ['کورونا','كوفيد',\n",
    "    'المغتربين', 'العمالة الوافدة','عامل وافد', 'العمال المهاجرين', 'العمال الوطنيين', 'العمال غير المواطنين' ,\n",
    " 'مواطن', 'غير مواطن'  , 'مقيم' , 'ساكن', 'التطعيم ضد كورونا', 'لقاح كورونا', 'الحجر صحي', 'دخول المستشفى' ]\n",
    "\n",
    "query_terms_en = ['Coronavirus', 'Covid-19', 'Vaccine', 'Immunization', 'expat worker', 'expats',\n",
    " 'expat workers', 'migrant workers', 'national workers', 'non-national workers',\n",
    "  'citizen', 'non-citizen', 'resident', 'dweller', 'Quarantine', 'hospitalize']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydir = './data_jan26/'\n",
    "if not os.path.exists(mydir):\n",
    "    os.mkdir(mydir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='test.csv'\n",
    "file_path = mydir + filename\n",
    "# df, tweet_cnt = user_query_tweets_to_csv(file_path, dubai_influencers, query_terms_ar[:2], count, since_date, until_date)\n",
    "test, tweet_cnt = user_query_tweets_to_csv(file_path, dubai_influencers, [''], count, since_date, until_date, 'ar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18468, 12)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(946, 12)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated extract all tweets, no keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Automated tool\n",
    "def generate_all_nokey(total_iter: dict, total_queries: dict, mydir, spaces = 1):\n",
    "\n",
    "    tz = time.perf_counter()\n",
    "    for iter in total_iter.keys():\n",
    "        with pd.ExcelWriter(mydir + f\"{iter}_report.xlsx\", engine='xlsxwriter') as writer:\n",
    "            for lang in total_queries.keys():\n",
    "                users = total_iter.get(iter)\n",
    "                keywords = ['']\n",
    "                filename = f\"{iter}_{lang}.csv\"\n",
    "                file_path = mydir + filename\n",
    "\n",
    "                df, tweet_cnt = user_query_tweets_to_csv(file_path, users, keywords, count, since_date, until_date, lang)\n",
    "                print(f\"Extracted tweets for following category {iter}_{lang}\")\n",
    "                print(f'Total Number of tweets : {df.shape[0]}')\n",
    "\n",
    "                # generating report -- more dfs could be added here\n",
    "                xx = pd.DataFrame(tweet_cnt, columns=[\n",
    "                                'keyword', 'handle', 'tweet count'])\n",
    "                df_handle = pd.DataFrame(users, columns=['User']).merge(xx.drop(columns='keyword').groupby(\n",
    "                    'handle').sum(), how='left', left_on='User', right_on='handle').fillna(0.0)\n",
    "                df_keywords = pd.DataFrame(keywords, columns=['keyword']).merge(xx.drop(\n",
    "                    columns='handle').groupby('keyword').sum(), how='left', on='keyword').fillna(0.0)\n",
    "\n",
    "                dfs = [df_handle, df_keywords]\n",
    "                col = 0\n",
    "                for dataframe in dfs:\n",
    "                    dataframe.to_excel(writer,sheet_name= f'counts_{lang}',startrow=1 , startcol=col)   \n",
    "                    # row = row + len(dataframe.index) + spaces + 1\n",
    "                    col = col + dataframe.shape[1] + spaces + 1\n",
    "\n",
    "\n",
    "    t = time.perf_counter() - tz\n",
    "    print(f\"Total Elapsed time to generate all iterations: {t:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted tweets for following category dubai_inf_ar\n",
      "Total Number of tweets : 18462\n",
      "Extracted tweets for following category dubai_inf_en\n",
      "Total Number of tweets : 14821\n",
      "Extracted tweets for following category dubai_gov_ar\n",
      "Total Number of tweets : 26354\n",
      "Extracted tweets for following category dubai_gov_en\n",
      "Total Number of tweets : 19086\n",
      "Total Elapsed time to generate all iterations: 2229.37s\n"
     ]
    }
   ],
   "source": [
    "OUTPUT = './data_march2nd/'\n",
    "if not os.path.exists(OUTPUT):\n",
    "    os.mkdir(OUTPUT)\n",
    "\n",
    "since_date = '2020-01-03'\n",
    "until_date = '2022-06-03'\n",
    "# total tweets extracted for each user\n",
    "count = 10000\n",
    "\n",
    "\n",
    "total_iter = {\n",
    "    # \"kuwait_inf\": kuwait_influencers,\n",
    "    # \"kuwait_gov\": kuwait_gov,\n",
    "    \"dubai_inf\": dubai_influencers,\n",
    "    # \"dubai_gov\": dubai_gov\n",
    "}\n",
    "total_queries = {'ar': query_terms_ar, 'en': query_terms_en}\n",
    "\n",
    "generate_all_nokey(total_iter, total_queries, OUTPUT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated using keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report structure:\n",
    "\n",
    "Let's say for Kuwait influencers --- 4 total excel files at the end\n",
    "\n",
    "Consisted of multiple sheets:\n",
    "* count_en: query terms, usernames, total tweets extracted AND tweet_count\n",
    "* count_ar: query terms, usernames, total tweets extracted AND tweet_count\n",
    "* More info on users and number of likes?! *to ask stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of tweets in kuwait_inf_ar.csv: 7527\n"
     ]
    }
   ],
   "source": [
    "xx = pd.DataFrame(tweet_cnt, columns=['keyword', 'handle', 'tweet count'])\n",
    "print(f'Total Number of tweets in {filename}: {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>کورونا</td>\n",
       "      <td>@Alafasy</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>کورونا</td>\n",
       "      <td>@NabilAlawadhy</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کورونا</td>\n",
       "      <td>@DrAlnefisi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>کورونا</td>\n",
       "      <td>@TfTeeeSH</td>\n",
       "      <td>3371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>کورونا</td>\n",
       "      <td>@mbuyabis</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>لقاح كورونا</td>\n",
       "      <td>@Mishari77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>لقاح كورونا</td>\n",
       "      <td>@saeedtawfiqi</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>الحجر صحي</td>\n",
       "      <td>@TfTeeeSH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>دخول المستشفى</td>\n",
       "      <td>@TfTeeeSH</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>دخول المستشفى</td>\n",
       "      <td>@bourashed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           keyword          handle  tweet count\n",
       "0           کورونا        @Alafasy           20\n",
       "1           کورونا  @NabilAlawadhy           36\n",
       "2           کورونا     @DrAlnefisi            1\n",
       "3           کورونا       @TfTeeeSH         3371\n",
       "4           کورونا       @mbuyabis          188\n",
       "..             ...             ...          ...\n",
       "133    لقاح كورونا      @Mishari77            1\n",
       "134    لقاح كورونا   @saeedtawfiqi            6\n",
       "135      الحجر صحي       @TfTeeeSH            1\n",
       "136  دخول المستشفى       @TfTeeeSH            6\n",
       "137  دخول المستشفى      @bourashed            1\n",
       "\n",
       "[138 rows x 3 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated function to get tweets by group and generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Automated tool\n",
    "def generate_all(total_iter: dict, total_queries: dict, mydir, spaces = 1):\n",
    "\n",
    "    if not os.path.exists(mydir):\n",
    "        os.mkdir(mydir)\n",
    "\n",
    "    tz = time.perf_counter()\n",
    "    for iter in total_iter.keys():\n",
    "        with pd.ExcelWriter(mydir + f\"{iter}_report.xlsx\", engine='xlsxwriter') as writer:\n",
    "            for lang in total_queries.keys():\n",
    "                users = total_iter.get(iter)\n",
    "                keywords = total_queries.get(lang)\n",
    "                filename = f\"{iter}_{lang}.csv\"\n",
    "                file_path = mydir + filename\n",
    "\n",
    "                df, tweet_cnt = user_query_tweets_to_csv(file_path, users, keywords, count, since_date, until_date, lang)\n",
    "                print(f\"Extracted tweets for following category {iter}_{lang}\")\n",
    "                print(f'Total Number of tweets : {df.shape[0]}')\n",
    "\n",
    "                # generating report -- more dfs could be added here\n",
    "                xx = pd.DataFrame(tweet_cnt, columns=[\n",
    "                                'keyword', 'handle', 'tweet count'])\n",
    "                df_handle = pd.DataFrame(users, columns=['User']).merge(xx.drop(columns='keyword').groupby(\n",
    "                    'handle').sum(), how='left', left_on='User', right_on='handle').fillna(0.0)\n",
    "                df_keywords = pd.DataFrame(keywords, columns=['keyword']).merge(xx.drop(\n",
    "                    columns='handle').groupby('keyword').sum(), how='left', on='keyword').fillna(0.0)\n",
    "\n",
    "                dfs = [df_handle, df_keywords]\n",
    "                col = 0\n",
    "                for dataframe in dfs:\n",
    "                    dataframe.to_excel(writer,sheet_name= f'counts_{lang}',startrow=1 , startcol=col)   \n",
    "                    # row = row + len(dataframe.index) + spaces + 1\n",
    "                    col = col + dataframe.shape[1] + spaces + 1\n",
    "\n",
    "\n",
    "    t = time.perf_counter() - tz\n",
    "    print(f\"Total Elapsed time to generate all iterations: {t:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted tweets for following category dubai_inf_eng\n",
      "Total Number of tweets : 0\n",
      "Total Elapsed time to generate all iterations: 488.74s\n"
     ]
    }
   ],
   "source": [
    "# first test\n",
    "total_iter = {\n",
    "    # \"kuwait_inf\": kuwait_influencers,\n",
    "    # \"kuwait_gov\": kuwait_gov,\n",
    "    \"dubai_inf\": dubai_influencers,\n",
    "    # \"dubai_gov\": dubai_gov\n",
    "}\n",
    "# total_queries = {'ar': query_terms_ar, 'eng': query_terms_en}\n",
    "total_queries = {'eng': query_terms_en}\n",
    "\n",
    "mydir = './data_latest/'\n",
    "\n",
    "generate_all(total_iter, total_queries, mydir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d645c3317aaf09895bb0134fd8d00e89aa8dc5ffbd8ea5e957f40321832ab29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
